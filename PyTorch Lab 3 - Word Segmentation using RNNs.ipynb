{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch Lab 3 - Word Segmentation using RNNs.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_iRdVX6c5mGt","colab_type":"text"},"source":["# PyTorch Lab 3\n","\n","Written by Prachya Boonkwan (Arm)"]},{"cell_type":"code","metadata":{"id":"kc_wJrf85nT5","colab_type":"code","outputId":"11b232da-6abc-48f2-86fa-d92791cedfd6","executionInfo":{"status":"ok","timestamp":1540017490911,"user_tz":-420,"elapsed":3115,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!pip install -q torch torchvision tqdm nltk\n","import nltk\n","nltk.download('gutenberg')\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"ucZjh7KI5mGw","colab_type":"text"},"source":["## Sequence Prediction"]},{"cell_type":"markdown","metadata":{"id":"6DRFWtF19pA9","colab_type":"text"},"source":["In this workshop, we are going to implement a word tokenizer based on recurrent neural networks.\n","\n","We will apply sequence prediction to word segmentation. We are now going to simulate the problem in English by removing all spaces in English and pretending that it is a consecutively written language. For example, a sentence like\n","\n","`Hi there. My name is Arm.`\n","\n","would be converted into a string similar to below:\n","\n","`hithere.mynameisarm.`\n","\n","Then we will tokenize this string with a deep neural network model and compare the result with the original string."]},{"cell_type":"markdown","metadata":{"id":"gb8uytls5mG0","colab_type":"text"},"source":["## Header"]},{"cell_type":"markdown","metadata":{"id":"82xNj64z5mG1","colab_type":"text"},"source":["This part is the header of the code. My favorite import aliases for PyTorch are as follows. This will be very useful for speed coding."]},{"cell_type":"code","metadata":{"id":"OB_zhMoX5mG3","colab_type":"code","colab":{}},"source":["#!/usr/bin/env python3\n","#-*- coding: utf-8 -*-\n","\n","import torch as T\n","import torch.nn as N\n","import torch.optim as O\n","\n","from tqdm import tqdm    # Nice progressbar\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0DC9hNTK5mG-","colab_type":"text"},"source":["-----"]},{"cell_type":"markdown","metadata":{"id":"s04Bqg_75mG_","colab_type":"text"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"S8zpApCh5mHB","colab_type":"text"},"source":["### Using Free Dataset from The Gutenberg Project "]},{"cell_type":"markdown","metadata":{"id":"ySC05reSOGXK","colab_type":"text"},"source":["We will download a sample text from NLTK (natural language toolkit) and make it our dataset (a.k.a. *corpus*). Here we choose 'The Hamlet' of Shakespeare, a classic English novel from the 16th century. We convert every word into the lower case (e.g. `Hello` -> `hello`).\n","\n","Note that this dataset is textual, not numerical. We will have to convert it into a bunch of numbers so that it can be processed by the neural networks."]},{"cell_type":"code","metadata":{"id":"6u9RoHx25mHD","colab_type":"code","outputId":"45505868-e473-4d83-b59f-2e8e7b5caee5","executionInfo":{"status":"ok","timestamp":1540017493026,"user_tz":-420,"elapsed":890,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from nltk.corpus import gutenberg\n","\n","corpus = [ [word.lower() for word in sent]\n","           for sent in gutenberg.sents('shakespeare-hamlet.txt') ]\n","\n","crplen = len(corpus)\n","print('Number of sentences = {}'.format(crplen))\n","\n","no_words = sum(len(sent) for sent in corpus)\n","print('Number of words     = {}'.format(no_words))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of sentences = 3106\n","Number of words     = 37360\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZZWpry1N5mHM","colab_type":"text"},"source":["### Character Indexing"]},{"cell_type":"markdown","metadata":{"id":"xZaqs0gcOq3p","colab_type":"text"},"source":["Once we obtain the dataset, we will now convert it into sequences of numbers. Our objective is to convert a string e.g. `hello` into a vector of five numbers representing each character. Such numbers are called *character indices*.\n","\n","We first populate all characters in the dataset into `all_chars`."]},{"cell_type":"code","metadata":{"id":"Fht2AxNL5mHO","colab_type":"code","colab":{}},"source":["all_chars = set()\n","for sent in corpus:\n","    for word in sent:\n","        all_chars.update(set(word))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tje9OYqnPmYy","colab_type":"text"},"source":["Then we create a mapping table from an index to a corresponding character. Let's name this table `idx2char`."]},{"cell_type":"code","metadata":{"id":"QbWbBcu45mHS","colab_type":"code","outputId":"2408c504-45dc-4b7f-d3f5-978e2acba71c","executionInfo":{"status":"ok","timestamp":1540017495760,"user_tz":-420,"elapsed":827,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["idx2char = [None] + sorted(all_chars)\n","print(idx2char)\n","\n","no_chars = len(idx2char)\n","print('Number of characters = {}'.format(no_chars))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[None, '!', '&', \"'\", '(', ')', ',', '-', '.', '1', '5', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","Number of characters = 43\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NQzAaGbrPtz9","colab_type":"text"},"source":["Next we create another table that maps each character back to its corresponding index. Let's name this table `char2idx`."]},{"cell_type":"code","metadata":{"id":"yxDKPXf-5mHY","colab_type":"code","outputId":"9225ed8a-bab4-472a-d75a-42148f147907","executionInfo":{"status":"ok","timestamp":1540017496827,"user_tz":-420,"elapsed":961,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["char2idx = {}\n","for (idx, char) in enumerate(idx2char):\n","    char2idx[char] = idx\n","print(char2idx)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{None: 0, '!': 1, '&': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '1': 9, '5': 10, '9': 11, ':': 12, ';': 13, '?': 14, '[': 15, ']': 16, 'a': 17, 'b': 18, 'c': 19, 'd': 20, 'e': 21, 'f': 22, 'g': 23, 'h': 24, 'i': 25, 'j': 26, 'k': 27, 'l': 28, 'm': 29, 'n': 30, 'o': 31, 'p': 32, 'q': 33, 'r': 34, 's': 35, 't': 36, 'u': 37, 'v': 38, 'w': 39, 'x': 40, 'y': 41, 'z': 42}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aI-IAZ9jQGmY","colab_type":"text"},"source":["Note that we deliberately add `None` as one of the characters. It will be used for finding an unknown/unseen character to an index by mapping it to `None`."]},{"cell_type":"markdown","metadata":{"id":"XTb18K4h5mHd","colab_type":"text"},"source":["### Conversion between String and Index Sequence"]},{"cell_type":"markdown","metadata":{"id":"hpVm7EmfP8QC","colab_type":"text"},"source":["It is now very easy to convert a character sequence into a sequence of character index. Here we sequentially convert each character in the sequence into its corresponding index."]},{"cell_type":"code","metadata":{"id":"I4RM0ONl5mHe","colab_type":"code","colab":{}},"source":["def str2idxseq(charseq):\n","    idxseq = []\n","    for char in charseq:\n","        char = char.lower()\n","        if char in char2idx:\n","            idxseq.append(char2idx[char])\n","        else:\n","            idxseq.append(char2idx[None])\n","    return idxseq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oq6z-jv5mHi","colab_type":"code","outputId":"ea46f6d0-5ecf-4eea-a283-97b9fb9e0404","executionInfo":{"status":"ok","timestamp":1540017499276,"user_tz":-420,"elapsed":815,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cuter_idxseq = str2idxseq('CutEr')\n","print(cuter_idxseq)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[19, 37, 36, 21, 34]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KFpqcWLYQhdt","colab_type":"text"},"source":["Also, we can sequentially convert a sequence of character indices back to a character sequence. Note that any unknown character index will be converted to a space."]},{"cell_type":"code","metadata":{"id":"40RXwSPf5mHn","colab_type":"code","colab":{}},"source":["def idxseq2str(idxseq):\n","    charseq = []\n","    for idx in idxseq:\n","        if idx < len(idx2char):\n","            charseq.append(idx2char[idx])\n","        else:\n","            charseq.append(' ')\n","    return charseq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbmo_Sg_5mHq","colab_type":"code","outputId":"c74c7c8e-a099-40cb-cd8d-b6524455bfbc","executionInfo":{"status":"ok","timestamp":1540017501344,"user_tz":-420,"elapsed":951,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(idxseq2str(cuter_idxseq))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['c', 'u', 't', 'e', 'r']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-P8vK6Mc5mHu","colab_type":"text"},"source":["### Preparing Training and Testing Sets"]},{"cell_type":"markdown","metadata":{"id":"TO26iYOsQ2Yc","colab_type":"text"},"source":["Now it's time to prepare our dataset for the neural networks. We will convert a sequence of words into a representation containing a sequence of character indices and a sequence of word delimiters. For example, the list `['my', 'name']` will be converted into:\n","\n","- A sequence of character indices [`m`, `y`, `n`, `a`, `m`, `e`]\n","- A sequence of word delimiters $[\\mathbb{F}, \\mathbb{T}, \\mathbb{F}, \\mathbb{F}, \\mathbb{F}, \\mathbb{T}]$\n","\n","| m | y | n | a | m | e |\n","|:-:|:-:|:-:|:-:|:-:|:-:|\n","| F | T | F | F | F | T |"]},{"cell_type":"code","metadata":{"id":"AVaB1wj45mHv","colab_type":"code","colab":{}},"source":["def sent2data(sent):\n","    charidxs = []\n","    wordbrks = []\n","    for charseq in sent:\n","        idxs = str2idxseq(charseq)\n","        charidxs.extend(idxs)\n","        wordbrks.extend((len(idxs) - 1) * [False] + [True])\n","    return (charidxs, wordbrks)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cjX34HbefK7u","colab_type":"text"},"source":["Let's try it out with the list `['hello', 'world']`."]},{"cell_type":"code","metadata":{"id":"9E0PwaB05mHy","colab_type":"code","outputId":"8081c58b-d1ba-4015-bfb9-6bf494d6a414","executionInfo":{"status":"ok","timestamp":1540017503435,"user_tz":-420,"elapsed":836,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["sent = ['hello', 'world']\n","charidxs, wordbrks = sent2data(sent)\n","print(charidxs)\n","print(wordbrks)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[24, 21, 28, 28, 31, 39, 31, 34, 28, 20]\n","[False, False, False, False, True, False, False, False, False, True]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NxESG06yRxAS","colab_type":"text"},"source":["Then we convert the whole dataset into this representation."]},{"cell_type":"code","metadata":{"id":"vie67KpU5mH2","colab_type":"code","colab":{}},"source":["def corpus2dataset(corpus):\n","    dataset = []\n","    for sent in corpus:\n","        charidxs, wordbrks = sent2data(sent)\n","        dataset.append((charidxs, wordbrks))\n","    return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZHPL1iDA5mH4","colab_type":"code","outputId":"fca39dde-d921-4fc8-f878-5326debd4233","executionInfo":{"status":"ok","timestamp":1540017505550,"user_tz":-420,"elapsed":945,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["crp = [ ['hello', 'world'],\n","        ['my', 'name', 'is', 'arm'] ]\n","ds = corpus2dataset(crp)\n","print(ds)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[([24, 21, 28, 28, 31, 39, 31, 34, 28, 20], [False, False, False, False, True, False, False, False, False, True]), ([29, 41, 30, 17, 29, 21, 25, 35, 17, 34, 29], [False, True, False, False, False, True, False, True, False, False, True])]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oMXn2qt_R9BF","colab_type":"text"},"source":["FInally, we separate the dataset into two parts: the training set (80%), and the testing set (20%)."]},{"cell_type":"code","metadata":{"id":"xY07mHr95mH7","colab_type":"code","outputId":"82493e38-a4f6-44e9-f9a7-3c73b18efcbf","executionInfo":{"status":"ok","timestamp":1540017506589,"user_tz":-420,"elapsed":949,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["training_len = int(crplen * 0.8)           # 80% for training, 20% for testing\n","training_set = corpus2dataset(corpus[: training_len])\n","testing_set = corpus2dataset(corpus[training_len :])\n","print('Size of training set = {}'.format(len(training_set)))\n","print('Size of testing set = {}'.format(len(testing_set)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size of training set = 2484\n","Size of testing set = 622\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cAPfjULx5mH_","colab_type":"text"},"source":["-----"]},{"cell_type":"markdown","metadata":{"id":"-D1SloW2SkVv","colab_type":"text"},"source":["### Common Settings"]},{"cell_type":"code","metadata":{"id":"0rmIYHXYHQom","colab_type":"code","colab":{}},"source":["dim_charvec = 10\n","dim_trans = 5\n","no_rnn_layers = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gw7RHXAqSiq6","colab_type":"text"},"source":["-------"]},{"cell_type":"markdown","metadata":{"id":"xscQzhtz5mIA","colab_type":"text"},"source":["## Embedding Layer"]},{"cell_type":"markdown","metadata":{"id":"E8hyjjUaSMFD","colab_type":"text"},"source":["Embedding layer is used for mapping any sparse item, such as characters, words, and any symbols, into a vector. This vector is sometimes called *embedding vector*. The command `N.Embedding` creates an embedding layer containing a specified number of sparse items and their embedding vectors contain a specified number of elements. "]},{"cell_type":"code","metadata":{"id":"f64Wzpj-5mIB","colab_type":"code","outputId":"fd738f77-8b3d-43fd-efb4-aa469becbe96","executionInfo":{"status":"ok","timestamp":1540017508939,"user_tz":-420,"elapsed":1050,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["charemb_layer = N.Embedding(no_chars, dim_charvec)\n","print(charemb_layer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Embedding(43, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MYTzSOcTTC8p","colab_type":"text"},"source":["This is how we compute an embedding vector of each character in a string."]},{"cell_type":"code","metadata":{"id":"HLQJU0VO5mIG","colab_type":"code","outputId":"992ee412-f834-4633-8e3e-8c29e0f06205","executionInfo":{"status":"ok","timestamp":1540017509970,"user_tz":-420,"elapsed":875,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["charseq = 'hello'\n","\n","charidxs = str2idxseq(charseq)\n","print('Index sequence of \"{}\" is {}\\n'.format(charseq, charidxs))\n","\n","charvecs = charemb_layer(T.LongTensor(charidxs))\n","print('Embedding vectors for each character:\\n{}'.format(charvecs))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Index sequence of \"hello\" is [24, 21, 28, 28, 31]\n","\n","Embedding vectors for each character:\n","tensor([[ 1.3507, -0.4437, -0.3860, -0.8155, -0.3321,  0.1039,  0.1480, -0.2776,\n","          1.6632,  1.6283],\n","        [-0.0226,  0.6330,  0.9687,  1.0236, -0.0603, -1.2780,  0.2938, -0.6008,\n","          0.5145,  1.3794],\n","        [-0.9798, -0.1157,  0.6120, -0.5613,  0.0252,  0.0742,  0.5543, -0.1895,\n","         -1.9382, -1.2358],\n","        [-0.9798, -0.1157,  0.6120, -0.5613,  0.0252,  0.0742,  0.5543, -0.1895,\n","         -1.9382, -1.2358],\n","        [-0.5508, -0.8740,  0.1747, -0.1624, -0.5439,  0.0976, -0.7356,  0.2648,\n","         -1.5961,  0.8870]], grad_fn=<EmbeddingBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5V27cWkXTKyo","colab_type":"text"},"source":["Note that the embedding vector of two `l`'s are the same."]},{"cell_type":"markdown","metadata":{"id":"sdxDJUnkTTQY","colab_type":"text"},"source":["Although the result is a matrix, we can easily get access to each embedding vector by matrix indexing `[]`."]},{"cell_type":"code","metadata":{"id":"rU3hXAKp5mIJ","colab_type":"code","outputId":"d6761c8b-8a90-45ab-c5da-93c0534c92c9","executionInfo":{"status":"ok","timestamp":1540017511102,"user_tz":-420,"elapsed":1054,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["for i in range(len(charseq)):\n","    print('Character embedding for index {} \"{}\" is:\\n{}'.format(i, charseq[i], charvecs[i]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Character embedding for index 0 \"h\" is:\n","tensor([ 1.3507, -0.4437, -0.3860, -0.8155, -0.3321,  0.1039,  0.1480, -0.2776,\n","         1.6632,  1.6283], grad_fn=<SelectBackward>)\n","Character embedding for index 1 \"e\" is:\n","tensor([-0.0226,  0.6330,  0.9687,  1.0236, -0.0603, -1.2780,  0.2938, -0.6008,\n","         0.5145,  1.3794], grad_fn=<SelectBackward>)\n","Character embedding for index 2 \"l\" is:\n","tensor([-0.9798, -0.1157,  0.6120, -0.5613,  0.0252,  0.0742,  0.5543, -0.1895,\n","        -1.9382, -1.2358], grad_fn=<SelectBackward>)\n","Character embedding for index 3 \"l\" is:\n","tensor([-0.9798, -0.1157,  0.6120, -0.5613,  0.0252,  0.0742,  0.5543, -0.1895,\n","        -1.9382, -1.2358], grad_fn=<SelectBackward>)\n","Character embedding for index 4 \"o\" is:\n","tensor([-0.5508, -0.8740,  0.1747, -0.1624, -0.5439,  0.0976, -0.7356,  0.2648,\n","        -1.5961,  0.8870], grad_fn=<SelectBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3Sh4d5SI5mIM","colab_type":"text"},"source":["-----"]},{"cell_type":"markdown","metadata":{"id":"SyAvgNBy5mIN","colab_type":"text"},"source":["## Recurrent Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"MyhKi4KpTvKf","colab_type":"text"},"source":["A recurrent neural network (RNN) is a neural network that takes its internal state as an input of the next feed-forwarding. Doing so enables the neural network to capture long-range dependency of any input sequence, making sequence prediction possible.\n","\n","RNNs linearly sweep across an input sequence of vectors to produce context vectors. This allows RNNs to accept input sequences of variable lengths during each training session. However, it is a bit harder to run batch training, because the lengths of input sequences are not equal.\n","\n","In this workshop, we will therefore focus on online training, i.e. feeding one training datum and perform backpropagation at a time. We will come back to batch training of RNNs in Lab 5."]},{"cell_type":"markdown","metadata":{"id":"6LSHC77o5mIN","colab_type":"text"},"source":["### Common Parameters"]},{"cell_type":"code","metadata":{"id":"aBFMjUDo5mIO","colab_type":"code","colab":{}},"source":["dim_charvec = 10\n","dim_trans = 5\n","no_rnn_layers = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NuG-yD135mIQ","colab_type":"text"},"source":["### Unidirectional RNNs"]},{"cell_type":"markdown","metadata":{"id":"_fNT5M0BUOD4","colab_type":"text"},"source":["The simplest form of RNNs is a unidirectional RNN. It takes a sequence of sparse items as an input, sweeps from left to right, performs feed-forwarding, and produces two outputs:\n","\n","1. The context vector for each step of feed-forwarding\n","2. The last internal state vector of the RNN's cell\n","\n","In this example, we choose Gated Recurrent Unit (GRU) `N.GRU`. You can also replace it with Long Short-Term Memories (LSTM) `N.LSTM`. They are almost the same, but the first converges a bit faster."]},{"cell_type":"code","metadata":{"id":"TJtLIvdQ5mIQ","colab_type":"code","outputId":"145bb46a-3a41-42cb-baef-1254c0c6e09e","executionInfo":{"status":"ok","timestamp":1540017513257,"user_tz":-420,"elapsed":1028,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rnn_unidir = N.GRU(dim_charvec, dim_trans, no_rnn_layers, batch_first=True)\n","print(rnn_unidir)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GRU(10, 5, batch_first=True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yRfyA4_AJVbK","colab_type":"text"},"source":["Here we set `batch_first=True` because we want the first dimension (`dim=0`) to become the batch ID. For ease of understanding, we will feed a sequence input vectors as a single mini-batch in this workshop."]},{"cell_type":"markdown","metadata":{"id":"LldNTkyOU-t3","colab_type":"text"},"source":["To perform sequence learning, we first convert a sequence of character embedding vectors into a single mini-batch with the command `unsqueeze(0)`.\n","\n","Then we pass this mini-batch into an RNN. It will produce two outputs:\n","\n","1. The context vectors of each step of feed-forwarding\n","2. The last internal state vector of the RNN's cell\n","\n","Once we obtain the outputs, we finally remove the mini-batch by the command `squeeze(0)`, which eliminates the mini-batch dimension from the obtained tensor."]},{"cell_type":"code","metadata":{"id":"2tfCYde75mIU","colab_type":"code","outputId":"19de47c1-ebac-4503-814d-6ebdf0a6825e","executionInfo":{"status":"ok","timestamp":1540017514342,"user_tz":-420,"elapsed":889,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["ctxvecs_unidir, lasthids_unidir = rnn_unidir(charvecs.unsqueeze(0))\n","ctxvecs_unidir, lasthids_unidir = ctxvecs_unidir.squeeze(0), lasthids_unidir.squeeze(1)\n","print(ctxvecs_unidir)\n","print(lasthids_unidir)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[-0.2847,  0.0535,  0.1816, -0.0720, -0.2713],\n","        [-0.4444,  0.2203,  0.2559,  0.3473,  0.2781],\n","        [-0.3983,  0.6929,  0.0815,  0.0902,  0.2487],\n","        [-0.3369,  0.8325, -0.0446, -0.1193,  0.2016],\n","        [-0.1563,  0.6219, -0.0959, -0.2301,  0.0338]],\n","       grad_fn=<SqueezeBackward1>)\n","tensor([[-0.1563,  0.6219, -0.0959, -0.2301,  0.0338]], grad_fn=<SqueezeBackward1>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KkoESOlz5mIY","colab_type":"text"},"source":["### Bidirectional RNNs"]},{"cell_type":"markdown","metadata":{"id":"tiIL-WnBWFpz","colab_type":"text"},"source":["We can sweep the input sequence from both directions and perform feed-forwarding separately. Just specify the argument `bidirection=True` in the command `N.GRU`."]},{"cell_type":"code","metadata":{"id":"aNL7O-k45mIZ","colab_type":"code","outputId":"7980865d-d2a3-4c9a-9545-4d47a68d3084","executionInfo":{"status":"ok","timestamp":1540017515743,"user_tz":-420,"elapsed":1323,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rnn_bidir = N.GRU(dim_charvec, dim_trans, no_rnn_layers, batch_first=True, bidirectional=True)\n","print(rnn_bidir)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GRU(10, 5, batch_first=True, bidirectional=True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rLg0HCSfWsCy","colab_type":"text"},"source":["Note that the context vectors double the size. That is because they contains one half produced by sweeping from left to right and the other half produced by sweeping from right to left."]},{"cell_type":"code","metadata":{"id":"0CDzXkjG5mIc","colab_type":"code","outputId":"032e7a2f-1a15-4128-a6ab-cc98ed9e7b4f","executionInfo":{"status":"ok","timestamp":1540017516780,"user_tz":-420,"elapsed":872,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["ctxvecs_bidir, lasthids_bidir = rnn_bidir(charvecs.unsqueeze(0))\n","ctxvecs_bidir, lasthids_bidir = ctxvecs_bidir.squeeze(0), lasthids_bidir.squeeze(1)\n","print(ctxvecs_bidir)\n","print(lasthids_bidir)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 0.3593,  0.1358,  0.0344,  0.4160, -0.0005, -0.3207, -0.1452,  0.7221,\n","          0.1527,  0.5107],\n","        [ 0.5828,  0.1820,  0.1284,  0.3117,  0.3223, -0.5374,  0.3903,  0.3163,\n","          0.2415,  0.2647],\n","        [ 0.4029, -0.3067,  0.5452,  0.0846,  0.1338, -0.4812,  0.2701, -0.0470,\n","          0.3769, -0.6320],\n","        [ 0.3554, -0.4218,  0.6903, -0.0454,  0.0176, -0.5626,  0.0899,  0.0280,\n","          0.2820, -0.5268],\n","        [ 0.2736,  0.3794, -0.0294, -0.0843,  0.0964, -0.8405, -0.2844,  0.1390,\n","          0.0449, -0.2323]], grad_fn=<SqueezeBackward1>)\n","tensor([[ 0.2736,  0.3794, -0.0294, -0.0843,  0.0964],\n","        [-0.3207, -0.1452,  0.7221,  0.1527,  0.5107]],\n","       grad_fn=<SqueezeBackward1>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WkWENkgz5mIg","colab_type":"text"},"source":["### Multilayer Bidirectional RNNs"]},{"cell_type":"markdown","metadata":{"id":"41_ZrzJ_XCkG","colab_type":"text"},"source":["We can also specify the number of RNN layers for learning more abstract representation, e.g. character -> syllable -> morpheme -> word -> phrase -> sentence -> etc."]},{"cell_type":"code","metadata":{"id":"X4jFKXMT5mIh","colab_type":"code","outputId":"75e7066e-3d4b-443b-ae89-bdcc3c1433d4","executionInfo":{"status":"ok","timestamp":1540017517833,"user_tz":-420,"elapsed":998,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["rnn_bidir_layer2 = N.GRU(dim_charvec, dim_trans, 2, batch_first=True, bidirectional=True)\n","print(rnn_bidir_layer2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GRU(10, 5, num_layers=2, batch_first=True, bidirectional=True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XIAESULgXZSZ","colab_type":"text"},"source":["Note that the context vectors produced by the multilayer RNNs belong to the last layer. However, the last state vectors obtained contain all vectors of the layers."]},{"cell_type":"code","metadata":{"id":"wXmbu49s5mIj","colab_type":"code","outputId":"7f72178f-682a-4db5-d197-a293f234f6bb","executionInfo":{"status":"ok","timestamp":1540017518854,"user_tz":-420,"elapsed":966,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["ctxvecs_bidir_layer2, lasthids_bidir_layer2 = rnn_bidir_layer2(charvecs.unsqueeze(0))\n","ctxvecs_bidir_layer2, lasthids_bidir_layer2 = ctxvecs_bidir_layer2.squeeze(0), lasthids_bidir_layer2.squeeze(1)\n","print(ctxvecs_bidir_layer2)\n","print(lasthids_bidir_layer2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[-0.3063,  0.1186,  0.1275, -0.0673, -0.0631, -0.4033, -0.3049,  0.6013,\n","         -0.2333,  0.0012],\n","        [-0.4276,  0.2827, -0.0627, -0.2562, -0.1182, -0.2280, -0.0899,  0.4121,\n","         -0.2783, -0.0237],\n","        [-0.4592,  0.4508, -0.2386, -0.3302, -0.2208, -0.1792, -0.0434,  0.2179,\n","         -0.2962, -0.1194],\n","        [-0.4980,  0.5327, -0.3057, -0.3353, -0.3444, -0.2165, -0.1164,  0.2296,\n","         -0.2053, -0.1007],\n","        [-0.5414,  0.5382, -0.3445, -0.3370, -0.4800, -0.1979, -0.1411,  0.1918,\n","         -0.1410, -0.0975]], grad_fn=<SqueezeBackward1>)\n","tensor([[ 0.8053,  0.0007,  0.1434, -0.3563, -0.1532],\n","        [-0.2452, -0.4788,  0.5294, -0.2286,  0.1492],\n","        [-0.5414,  0.5382, -0.3445, -0.3370, -0.4800],\n","        [-0.4033, -0.3049,  0.6013, -0.2333,  0.0012]],\n","       grad_fn=<SqueezeBackward1>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I7XBCA2C5mIl","colab_type":"text"},"source":["-------"]},{"cell_type":"markdown","metadata":{"id":"dnFUzVET5mIm","colab_type":"text"},"source":["## Word Segmentation"]},{"cell_type":"markdown","metadata":{"id":"-8pG7rldX2vr","colab_type":"text"},"source":["Let's now observe an application of sequence prediction --- word segmentation."]},{"cell_type":"markdown","metadata":{"id":"b7RNd3205mIm","colab_type":"text"},"source":["### Neural Architecture"]},{"cell_type":"markdown","metadata":{"id":"bzAEf1fsYEVj","colab_type":"text"},"source":["The neural model for word segmentation used in this workshop is simple and practical. The neural architecture is as follows.\n","\n","1. Character embedding layer\n","2. Multilayer bidirectional RNNs\n","3. Hyperbolic tangent `N.Tanh`\n","4. Linear layer (hidden layer)\n","5. (Log) Softmax `N.LogSoftmax(dim=1)`"]},{"cell_type":"code","metadata":{"id":"2Z0kSAkv5mIn","colab_type":"code","colab":{}},"source":["class WordsegModel(N.Module):\n","    \n","    def __init__(self, dim_charvec, dim_trans, no_layers):\n","        super(WordsegModel, self).__init__()\n","        self._dim_charvec = dim_charvec\n","        self._dim_trans = dim_trans\n","        self._no_layers = no_layers\n","        \n","        self._charemb = N.Embedding(no_chars, self._dim_charvec)\n","        self._rnn = N.GRU(\n","            self._dim_charvec, self._dim_trans, self._no_layers,\n","            batch_first=True, bidirectional=True\n","        )\n","        self._tanh = N.Tanh()\n","        self._hidden = N.Linear(2 * self._dim_trans, 2)    # Predicting two classes: break / no break\n","        self._log_softmax = N.LogSoftmax(dim=1)\n","        \n","    def forward(self, charidxs):\n","        try:\n","            charvecs = self._charemb(T.LongTensor(charidxs))\n","            # print('charvecs =\\n{}'.format(charvecs))\n","            ctxvecs, lasthids = self._rnn(charvecs.unsqueeze(0))\n","            ctxvecs, lasthids = ctxvecs.squeeze(0), lasthids.squeeze(1)\n","            # print('ctxvecs =\\n{}'.format(ctxvecs))\n","            statevecs = self._hidden(self._tanh(ctxvecs))\n","            # print('statevecs =\\n{}'.format(statevecs))\n","            brkvecs = self._log_softmax(statevecs)\n","            # print('brkvecs =\\n{}'.format(brkvecs))\n","            return brkvecs\n","        except RuntimeError:\n","            raise RuntimeError(statevecs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wa-hEYP5gI5m","colab_type":"text"},"source":["Let's create a word segmentation model with the following configuration.\n","\n","- Dimension of character embedding = 16\n","- Dimension of GRU's transition vector = 32\n","- Number of GRU layers = 2"]},{"cell_type":"code","metadata":{"id":"rjf4kzpR5mIo","colab_type":"code","colab":{}},"source":["wordseg_model = WordsegModel(dim_charvec=16, dim_trans=32, no_layers=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHLHW8c-ZFE_","colab_type":"text"},"source":["The output of this model is a sequence of word delimiter vectors. These vectors contains two components:\n","\n","1. The log-probability of breaking\n","2. The log-probability of not breaking"]},{"cell_type":"code","metadata":{"id":"f-sNVWBf5mIp","colab_type":"code","outputId":"7939d599-4635-49e8-8cc4-74e2d867de41","executionInfo":{"status":"ok","timestamp":1540017521592,"user_tz":-420,"elapsed":992,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["charidxs, wordbrks = sent2data(['my', 'name'])\n","brkvecs = wordseg_model(charidxs)\n","print(brkvecs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[-0.8702, -0.5428],\n","        [-0.8488, -0.5585],\n","        [-0.9205, -0.5080],\n","        [-0.8663, -0.5456],\n","        [-0.8666, -0.5454],\n","        [-0.7899, -0.6049]], grad_fn=<LogSoftmaxBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-5AUXyf75mIq","colab_type":"text"},"source":["### Converting to a Sequence of Prediction Classes"]},{"cell_type":"markdown","metadata":{"id":"531YKcgNZjSm","colab_type":"text"},"source":["Last we have to convert a sequence of word delimiters into a sequence of delimiter classes (`True` for breaking and `False` for not breaking)."]},{"cell_type":"code","metadata":{"id":"a08K9x_K5mIr","colab_type":"code","colab":{}},"source":["def wordbrks2brkvec(wordbrks):\n","    brkvec = T.LongTensor(len(wordbrks))\n","    for i in range(len(wordbrks)):\n","        if wordbrks[i]: brkvec[i] = 0\n","        else: brkvec[i] = 1\n","    return brkvec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVoa2Bu_5mIs","colab_type":"code","outputId":"38cf279f-3564-4e3f-9821-47de294b70d7","executionInfo":{"status":"ok","timestamp":1540017523729,"user_tz":-420,"elapsed":1020,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(wordbrks)\n","gold_brkvec = wordbrks2brkvec(wordbrks)\n","print(gold_brkvec)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[False, True, False, False, False, True]\n","tensor([1, 0, 1, 1, 1, 0])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pLaNL-Md5mIt","colab_type":"text"},"source":["### Negative Log Loss"]},{"cell_type":"markdown","metadata":{"id":"vPYB1IYIZ05s","colab_type":"text"},"source":["Negative Log Loss (NLL) is a loss function that shows how far the probability of the desired class is from 1. Let $\\mathbf{y}$ be a gold-standard vector and $\\mathbf{y}'$ be a predicted vector. Each $c$-th element of these vectors belongs to each class.\n","\n","$$\n","\\begin{eqnarray}\n","  \\mathrm{NLL} & = & - \\sum_{c=1}^M y_{c} \\log y'_{c}\n","\\end{eqnarray}\n","$$\n","\n","The negative log loss outperforms the mean squared error (MSE) in the case where the sizes of $\\mathbf{y}$ and $\\mathbf{y}'$ are very large. In MSE, we have to compute the difference between each pair of elements in both vectors. In NLL, however, we compute the difference only for the desired class, therefore boosting the speed."]},{"cell_type":"code","metadata":{"id":"Rk8c0hal5mIt","colab_type":"code","outputId":"47d7242d-9869-46cb-ade7-98caaf771517","executionInfo":{"status":"ok","timestamp":1540017524764,"user_tz":-420,"elapsed":973,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["loss_fn = N.NLLLoss()\n","loss_fn(brkvecs, gold_brkvec)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.6301, grad_fn=<NllLossBackward>)"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"markdown","metadata":{"id":"kBPoyuxJ5mIv","colab_type":"text"},"source":["-------"]},{"cell_type":"markdown","metadata":{"id":"xHhtyxm55mIw","colab_type":"text"},"source":["### Online Training"]},{"cell_type":"markdown","metadata":{"id":"B0q9nGIbcvi5","colab_type":"text"},"source":["Let just imitate the training procedure of Lab 2. :P"]},{"cell_type":"code","metadata":{"id":"KdiYkSqy5mIw","colab_type":"code","colab":{}},"source":["def train_model(wordseg_model, training_data, epochs, loss_fn, optimizer):\n","    no_samples = len(training_data)\n","    loss_history = []\n","    \n","    for i in range(epochs):\n","        total_loss = 0.0\n","        random.shuffle(training_data)\n","        \n","        for (charidxs, wordbrks) in tqdm(training_data):                \n","            pred_brkvecs = wordseg_model(charidxs)       # Perform prediction\n","            gold_brkvec = wordbrks2brkvec(wordbrks)      # Gold standard\n","            \n","            loss = loss_fn(pred_brkvecs, gold_brkvec)\n","            total_loss += loss.item()\n","            \n","            optimizer.zero_grad()      # Clear gradient cache\n","            loss.backward()            # Perform backpropagation\n","            optimizer.step()           # Update the model parameters\n","        \n","        loss_history.append(total_loss / no_samples)\n","        \n","    # Plot the loss history with Matplotlib\n","    epoch_count = range(1, epochs + 1)\n","    plt.plot(epoch_count, loss_history, 'b--')\n","    plt.legend(['Training Loss'])\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TnfTLa4Ac9N-","colab_type":"text"},"source":["<font color=\"red\">**PLEASE NOTE THAT ONLINE TRAINING IS TIME-CONSUMING.**</font>"]},{"cell_type":"code","metadata":{"id":"SHrMlVd35mIy","colab_type":"code","outputId":"517b8134-e743-4809-d991-d112097f97bd","executionInfo":{"status":"ok","timestamp":1540019342731,"user_tz":-420,"elapsed":1816893,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":449}},"source":["epochs = 10\n","# epochs = 20      # Only if you can wait\n","learning_rate = 0.001\n","\n","loss_fn = N.NLLLoss()\n","optimizer = O.Adam(wordseg_model.parameters(), lr=learning_rate)\n","train_model(wordseg_model, training_set, epochs, loss_fn, optimizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 2484/2484 [03:02<00:00,  8.25it/s]\n","100%|██████████| 2484/2484 [03:01<00:00, 13.71it/s]\n","100%|██████████| 2484/2484 [03:01<00:00, 13.68it/s]\n","100%|██████████| 2484/2484 [03:02<00:00, 10.07it/s]\n","100%|██████████| 2484/2484 [03:02<00:00, 13.60it/s]\n","100%|██████████| 2484/2484 [03:00<00:00, 13.76it/s]\n","100%|██████████| 2484/2484 [03:01<00:00, 13.69it/s]\n","100%|██████████| 2484/2484 [03:00<00:00, 13.78it/s]\n","100%|██████████| 2484/2484 [03:01<00:00, 13.69it/s]\n","100%|██████████| 2484/2484 [03:01<00:00, 10.50it/s]\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7f059cc39ef0>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"IucYW7kq5mIz","colab_type":"text"},"source":["-----"]},{"cell_type":"markdown","metadata":{"id":"oQ_9KpAx5mI0","colab_type":"text"},"source":["### Trying out"]},{"cell_type":"code","metadata":{"id":"7Fi2wdaT5mI0","colab_type":"code","colab":{}},"source":["def tokenize(wordseg_model, charseq):\n","    charidxs = str2idxseq(charseq)\n","    pred_brkvecs = wordseg_model(charidxs)\n","    pred_wordbrks = []\n","    for i in range(len(charidxs)):\n","        pred_wordbrk = (pred_brkvecs[i][0] > pred_brkvecs[i][1])\n","        # print(pred_wordbrk)\n","        pred_wordbrks.append(pred_wordbrk)\n","    \n","    sent = []\n","    word = []\n","    begpos = 0\n","    for i in range(len(pred_wordbrks)):\n","        if pred_wordbrks[i]:\n","            word.append(charseq[i])\n","            sent.append(word)\n","            word = []\n","            begpos = i\n","        else:\n","            word.append(charseq[i])\n","    if len(word) > 0: sent.append(word)\n","        \n","    return sent"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWToNFVCGWuh","colab_type":"code","outputId":"bdbb09a8-f889-43d7-a1d0-cea346bdd38d","executionInfo":{"status":"ok","timestamp":1540019344922,"user_tz":-420,"elapsed":1041,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["words = tokenize(wordseg_model, 'tobe,ornottobe,thatisthequestion.')\n","print(' '.join(''.join(word) for word in words))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["to be , or not to be , that is the question .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nQjIi42d5mI1","colab_type":"text"},"source":["-------"]},{"cell_type":"markdown","metadata":{"id":"Ov3YGt6G5mI1","colab_type":"text"},"source":["### Testing Procedure"]},{"cell_type":"markdown","metadata":{"id":"-dR_5UQzdHBn","colab_type":"text"},"source":["Let's test our model and how much accuracy we can achieve."]},{"cell_type":"code","metadata":{"id":"XHt9EKHy5mI2","colab_type":"code","colab":{}},"source":["def test_model(wordseg_model, testing_data):\n","    no_correct = 0.0\n","    no_goldbrks = 0.0\n","    no_predbrks = 0.0\n","\n","    for (charidxs, gold_wordbrks) in tqdm(testing_data):\n","        pred_brkvecs = wordseg_model(charidxs)\n","        pred_wordbrks = []\n","        for i in range(len(charidxs)):\n","            pred_wordbrk = (pred_brkvecs[i][0] > pred_brkvecs[i][1])\n","            pred_wordbrks.append(pred_wordbrk)\n","        \n","        for i in range(len(charidxs)):\n","            if gold_wordbrks[i] and gold_wordbrks[i] == pred_wordbrks[i]:\n","                no_correct += 1.0\n","            if gold_wordbrks[i]:\n","                no_goldbrks += 1.0\n","            if pred_wordbrks[i]:\n","                no_predbrks += 1.0\n","\n","    precision = 100 * no_correct / no_predbrks\n","    recall = 100 * no_correct / no_goldbrks\n","    f1 = 2 * precision * recall / (precision + recall)\n","    \n","    print('\\nPrecision = {}'.format(precision))\n","    print('Recall = {}'.format(recall))\n","    print('F1 = {}'.format(f1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNBemgRQ5mI3","colab_type":"code","outputId":"e05931a7-aaba-4533-c929-e722e42b45f8","executionInfo":{"status":"ok","timestamp":1540019363334,"user_tz":-420,"elapsed":17302,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["test_model(wordseg_model, testing_set)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 622/622 [00:16<00:00, 38.26it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Precision = 94.6896252351324\n","Recall = 95.03340110368865\n","F1 = 94.86120171051677\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"u4WPeANWfAIW","colab_type":"text"},"source":["## Exercises"]},{"cell_type":"markdown","metadata":{"id":"wbCxsdeVelaw","colab_type":"text"},"source":["Vary the dimension of character vectors, the dimension of internal states, and the number of RNN layers. Observe the change of F1 scores.\n","\n","<font color=\"red\">**PLEASE NOTE THAT ONLINE TRAINING IS TIME-CONSUMING.**</font>"]},{"cell_type":"code","metadata":{"id":"_nLw4RKh5mI4","colab_type":"code","colab":{}},"source":["# for dim_charvec in [4, 8, 16, 32, 64]:\n","#     for dim_trans in [4, 8, 16, 32, 64]:\n","#         for no_layers in [1, 2, 3]:\n","#             print('>>> dim_charvec={}  dim_trans={}  no_layers={}'.format(dim_charvec, dim_trans, no_layers))\n","#             wordseg_model_ex1 = WordsegModel(dim_charvec, dim_trans, no_layers)\n","#             train_model(wordseg_model_ex1, training_set, epochs, loss_fn, optimizer)\n","#             test_model(wordseg_model_ex1, testing_set)"],"execution_count":0,"outputs":[]}]}