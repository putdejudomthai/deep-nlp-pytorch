{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch Lab 5 - Movie Review Classification using HANs.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"wGB7vgQLhRU3","colab_type":"text"},"source":["# PyTorch Lab 5\n","\n","Written by Prachya Boonkwan (Arm)"]},{"cell_type":"code","metadata":{"id":"_ZblBuEUhimj","colab_type":"code","outputId":"14d3e622-fb12-4d68-9bbb-180e1c0af9ab","executionInfo":{"status":"ok","timestamp":1547779364984,"user_tz":-420,"elapsed":23230,"user":{"displayName":"Prachya Boonkwan","photoUrl":"https://lh6.googleusercontent.com/-dxQvofnVYg0/AAAAAAAAAAI/AAAAAAAACBE/Fo9VHYdR6t4/s64/photo.jpg","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!pip install -q torch torchvision tqdm nltk\n","import nltk\n","nltk.download('movie_reviews')\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/movie_reviews.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"IS2Yu4efhxnq","colab_type":"text"},"source":["## Header"]},{"cell_type":"markdown","metadata":{"id":"KsD3c2i5h5Uq","colab_type":"text"},"source":["This part is the header of the code. My favorite import aliases for PyTorch are as follows. This will be very useful for speed coding."]},{"cell_type":"code","metadata":{"id":"OBireOEGhzfW","colab_type":"code","outputId":"fe644c29-41a8-4c4c-c302-a34d1ff427eb","executionInfo":{"status":"ok","timestamp":1547779364986,"user_tz":-420,"elapsed":22983,"user":{"displayName":"Prachya Boonkwan","photoUrl":"https://lh6.googleusercontent.com/-dxQvofnVYg0/AAAAAAAAAAI/AAAAAAAACBE/Fo9VHYdR6t4/s64/photo.jpg","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#!/usr/bin/env python3\n","#-*- coding: utf-8 -*-\n","\n","import torch as T\n","import torch.nn as N\n","import torch.optim as O\n","\n","from tqdm import tqdm    # Nice progressbar\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import random\n","\n","if T.cuda.is_available():\n","    print('Feel free to use GPU acceleration.')\n","else:\n","    print('GPU is not available at this time.')\n","\n","use_cuda = True   # False"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GPU is not available at this time.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qlSF1k9ph9h5","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RmO-hTKjh-VX","colab_type":"text"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"SKGyj5lGiGKw","colab_type":"text"},"source":["### Using Free Dataset from NLTK's Movie Reviews"]},{"cell_type":"code","metadata":{"id":"-3xaUEm2holv","colab_type":"code","outputId":"5a6c7f9e-bad8-4a31-a382-7cdadb0d4fbd","executionInfo":{"status":"ok","timestamp":1540059729155,"user_tz":-420,"elapsed":5529,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from nltk.corpus import movie_reviews\n","\n","corpus = []\n","for category in movie_reviews.categories():\n","    for fileid in movie_reviews.fileids(category):\n","        corpus.append((\n","            [ list(sent) for sent in movie_reviews.sents(fileid) ],\n","            category\n","        ))\n","random.shuffle(corpus)\n","\n","crplen = len(corpus)\n","print('Number of movie reviews = {}'.format(crplen))\n","\n","no_words = sum(\n","    len(sent)\n","    for (sents, category) in corpus\n","    for sent in sents\n",")\n","print('Number of words         = {}'.format(no_words))\n","\n","max_sentlen = max(\n","    len(sent)\n","    for (sents, category) in corpus\n","    for sent in sents\n",")\n","print('Max sentence length     = {}'.format(max_sentlen))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of movie reviews = 2000\n","Number of words         = 1583820\n","Max number of words     = 187\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bHM3j0wJjtVd","colab_type":"text"},"source":["### Indexing Words and Intents"]},{"cell_type":"code","metadata":{"id":"Z70AWEOKjjbH","colab_type":"code","colab":{}},"source":["all_words = set()\n","all_categories = set()\n","for (sents, category) in corpus:\n","    for sent in sents:\n","        all_words.update(set(sent))\n","    all_categories.add(category)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hada1nsRj0k3","colab_type":"code","outputId":"0963697d-0032-4056-ac7c-f432c99f636b","executionInfo":{"status":"ok","timestamp":1540059731908,"user_tz":-420,"elapsed":1135,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["idx2word = [None] + sorted(all_words)\n","\n","idx2category = sorted(all_categories)\n","\n","no_words = len(idx2word)\n","no_categories = len(idx2category)\n","print('Number of words      = {}'.format(no_words))\n","print('Number of categories = {}'.format(no_categories))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of words      = 39769\n","Number of categories = 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mFzeFXvSljVJ","colab_type":"code","outputId":"3261bd17-7a71-45b0-81b5-d171a2e33e88","executionInfo":{"status":"ok","timestamp":1540059734504,"user_tz":-420,"elapsed":2436,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["word2idx = {}\n","for (idx, word) in enumerate(idx2word):\n","    word2idx[word] = idx\n","print(len(word2idx))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["39769\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I0GMy-LvlyJF","colab_type":"code","outputId":"a38f75d4-5712-419c-b1a4-90c7b9f0740d","executionInfo":{"status":"ok","timestamp":1540059735797,"user_tz":-420,"elapsed":1247,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["category2idx = {}\n","for (idx, category) in enumerate(idx2category):\n","    category2idx[category] = idx\n","# print(len(category2idx))\n","print(category2idx)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'neg': 0, 'pos': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ozrDQ6E2m4fS","colab_type":"text"},"source":["### Conversion between Sentence and Index Sequence"]},{"cell_type":"code","metadata":{"id":"yTloXqy2m5iN","colab_type":"code","colab":{}},"source":["def sent2idxseq(sent):\n","    wordidxseq = []\n","    for word in sent:\n","        word = word.lower()\n","        if word in word2idx:\n","            wordidxseq.append(word2idx[word])\n","        else:\n","            wordidxseq.append(word2idx[None])\n","    return wordidxseq\n","\n","def sents2idxseqs(sents):\n","    return [sent2idxseq(sent) for sent in sents]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"krguz-4VnKr3","colab_type":"code","outputId":"64757f2c-5452-4e69-9104-c21d3d158ab4","executionInfo":{"status":"ok","timestamp":1540059738593,"user_tz":-420,"elapsed":1565,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["wordidxseq1 = sent2idxseq('This is a good movie .'.split())\n","print(wordidxseq1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[35496, 18672, 747, 15090, 23206, 39]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gsA3JsWMnX5L","colab_type":"code","colab":{}},"source":["def idxseq2sent(wordidxseq):\n","    result = []\n","    for i in range(len(wordidxseq)):\n","        wordidx = wordidxseq[i]\n","        if wordidx < len(idx2word):\n","            word = idx2word[wordidx]\n","            if word is not None:\n","                result.append(word)\n","    return ' '.join(result)\n","\n","def idxseqs2sents(wordidxseqs):\n","    return [idxseq2sent(wordidxseq) for wordidxseq in wordidxseqs]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Pfhx0V1nadK","colab_type":"code","outputId":"8bf61694-78c5-44bd-d986-a2c5e557f5d0","executionInfo":{"status":"ok","timestamp":1540059741672,"user_tz":-420,"elapsed":1846,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["s = idxseq2sent(wordidxseq1)\n","print(s)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["this is a good movie .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"smWCYgkJnz6u","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5MFoB5H0qAEy","colab_type":"text"},"source":["### Preparing Training and Testing Sets"]},{"cell_type":"code","metadata":{"id":"TAg8zmXXn09r","colab_type":"code","colab":{}},"source":["def corpus2dataset(corpus):\n","    dataset = []\n","    for (sents, category) in corpus:\n","        wordidxseqs = sents2idxseqs(sents)\n","        catidx = category2idx[category]\n","        dataset.append((wordidxseqs, catidx))\n","    return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6el38BKgl7Ez","colab_type":"code","outputId":"a3774c31-e305-4561-ed32-00bb4f8652d3","executionInfo":{"status":"ok","timestamp":1540059745082,"user_tz":-420,"elapsed":1520,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["crp = [ ([ 'this is a good movie'.split(),\n","           'worth every cent'.split() ], \n","         'pos'),\n","        ([ 'this is a bad movie'.split(),\n","           'ruined my memories'.split() ], \n","         'neg') ]\n","ds = corpus2dataset(crp)\n","print(ds)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[([[35496, 18672, 747, 15090, 23206], [39299, 12238, 5963]], 1), ([[35496, 18672, 747, 3036, 23206], [30204, 23444, 22215]], 0)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1o2rVNyLrNM0","colab_type":"code","outputId":"aeaa4ee6-f0f2-4d2e-d437-1465897da6d9","executionInfo":{"status":"ok","timestamp":1540059747156,"user_tz":-420,"elapsed":1899,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["training_len = int(crplen * 0.8)           # 80% for training, 20% for testing\n","training_set = corpus2dataset(corpus[: training_len])\n","testing_set = corpus2dataset(corpus[training_len :])\n","print('Size of training set = {}'.format(len(training_set)))\n","print('Size of testing set = {}'.format(len(testing_set)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size of training set = 1600\n","Size of testing set = 400\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AaeqUrQPtN1B","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CqZNlceDtO72","colab_type":"text"},"source":["## Hierarchical Attention Networks"]},{"cell_type":"markdown","metadata":{"id":"wvTG1yoltYM1","colab_type":"text"},"source":["### Attention Mechanism"]},{"cell_type":"markdown","metadata":{"id":"rUgDqOU3620x","colab_type":"text"},"source":["Attention mechanism can be seen as computing the weighted average of input vectors, where weighting can be automatically trained with the dataset. Unfortunately the attention mechanism is not available on PyTorch at the moment (version 1.0), so we have to implement it by ourselves.\n","\n","In this implementation, we assume that input vectors are $ \\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\ldots, \\mathbf{v}_M $, whose sizes are $ N $ dimension. The score of each vector is computed by:\n","\\begin{eqnarray}\n","    s_k & = & \\tanh (\\mathbf{a} \\cdot \\mathbf{v}_k)\n","\\end{eqnarray}\n","where vector $ \\mathbf{a} $ is the kernel vector. Since each $-1 \\leq s_k \\leq 1$, we then compute the probabilistic weight of each vector by the softmax function:\n","\\begin{eqnarray}\n","    w_k & = & \\frac{\\exp s_k}{\\sum_{m = 1}^M \\exp s_m}\n","\\end{eqnarray}\n","The result of the attention mechanism is the weighted average of all input vectors with respect to the kernel vector $ \\mathbf{a} $. Therefore:\n","\\begin{eqnarray}\n","    \\textrm{Attention}(\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3, \\ldots, \\mathbf{v}_M | \\mathbf{a}) & = & \\sum_{k = 1}^M w_k \\mathbf{v}_k \n","\\end{eqnarray}"]},{"cell_type":"code","metadata":{"id":"oi4ij3detbej","colab_type":"code","colab":{}},"source":["class Attention(N.Module):\n","    \n","    def __init__(self, dim_input, batch_first=True):\n","        super(Attention, self).__init__()\n","        self._dim_input = dim_input\n","        self._batch_first = batch_first\n","        \n","        self._kernel = N.Linear(self._dim_input, 1)\n","        self._tanh = N.Tanh()\n","        if batch_first:\n","            self._softmax = N.Softmax(dim=1)\n","        else:\n","            self._softmax = N.Softmax(dim=0)\n","        \n","    def forward(self, vecs):\n","        scores = self._tanh(self._kernel(vecs))\n","        print('Scores:\\n{}'.format(scores))\n","        weights = self._softmax(scores)\n","        print('Weights:\\n{}'.format(weights))\n","        weighted_vecs = vecs * weights\n","        print('Weighted vectors:\\n{}'.format(weighted_vecs))\n","        if self._batch_first:\n","            out = weighted_vecs.sum(dim=1)\n","        else:\n","            out = weighted_vecs.sum(dim=0)\n","        print('Output:\\n{}'.format(out))\n","        return out\n","    \n","    def repr(self):\n","        return 'Attn(dim_input={}, batch_first={})'.format(\n","            self._dim_input, self._batch_first\n","        )\n","    \n","    def __repr__(self):\n","        return self.repr()\n","    \n","    def __str__(self):\n","        return self.repr()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mg-NYXHnqyl7","colab_type":"code","outputId":"545989f6-8a34-488d-e8db-712daee84663","executionInfo":{"status":"ok","timestamp":1547779384643,"user_tz":-420,"elapsed":1115,"user":{"displayName":"Prachya Boonkwan","photoUrl":"https://lh6.googleusercontent.com/-dxQvofnVYg0/AAAAAAAAAAI/AAAAAAAACBE/Fo9VHYdR6t4/s64/photo.jpg","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":1003}},"source":["attn = Attention(5)\n","print(attn)\n","\n","# batch ID is on dim 0\n","batch = T.FloatTensor([ [ [ 1,  2,  3,  4,  5],\n","                          [ 6,  7,  8,  9, 10],\n","                          [11, 12, 13, 14, 15] ],\n","                       \n","                        [ [16, 17, 18, 19, 20],\n","                          [21, 22, 23, 24, 25],\n","                          [26, 27, 28, 29, 30] ],\n","                       \n","                        [ [31, 32, 33, 34, 35],\n","                          [36, 37, 38, 39, 40],\n","                          [41, 42, 43, 44, 45] ]\n","                      ])\n","print(batch)\n","\n","out = attn(batch)\n","print(out)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Attn(dim_input=5, batch_first=True)\n","tensor([[[ 1.,  2.,  3.,  4.,  5.],\n","         [ 6.,  7.,  8.,  9., 10.],\n","         [11., 12., 13., 14., 15.]],\n","\n","        [[16., 17., 18., 19., 20.],\n","         [21., 22., 23., 24., 25.],\n","         [26., 27., 28., 29., 30.]],\n","\n","        [[31., 32., 33., 34., 35.],\n","         [36., 37., 38., 39., 40.],\n","         [41., 42., 43., 44., 45.]]])\n","Scores:\n","tensor([[[-0.8429],\n","         [-0.9956],\n","         [-0.9999]],\n","\n","        [[-1.0000],\n","         [-1.0000],\n","         [-1.0000]],\n","\n","        [[-1.0000],\n","         [-1.0000],\n","         [-1.0000]]], grad_fn=<TanhBackward>)\n","Weights:\n","tensor([[[0.3686],\n","         [0.3164],\n","         [0.3150]],\n","\n","        [[0.3333],\n","         [0.3333],\n","         [0.3333]],\n","\n","        [[0.3333],\n","         [0.3333],\n","         [0.3333]]], grad_fn=<SoftmaxBackward>)\n","Weighted vectors:\n","tensor([[[ 0.3686,  0.7372,  1.1058,  1.4743,  1.8429],\n","         [ 1.8983,  2.2147,  2.5311,  2.8474,  3.1638],\n","         [ 3.4654,  3.7804,  4.0954,  4.4105,  4.7255]],\n","\n","        [[ 5.3333,  5.6667,  6.0000,  6.3333,  6.6667],\n","         [ 7.0000,  7.3333,  7.6667,  8.0000,  8.3333],\n","         [ 8.6667,  9.0000,  9.3333,  9.6667, 10.0000]],\n","\n","        [[10.3333, 10.6667, 11.0000, 11.3333, 11.6667],\n","         [12.0000, 12.3333, 12.6667, 13.0000, 13.3333],\n","         [13.6667, 14.0000, 14.3333, 14.6667, 15.0000]]],\n","       grad_fn=<ThMulBackward>)\n","Output:\n","tensor([[ 5.7322,  6.7322,  7.7322,  8.7322,  9.7322],\n","        [21.0000, 22.0000, 23.0000, 24.0000, 25.0000],\n","        [36.0000, 37.0000, 38.0000, 39.0000, 40.0000]],\n","       grad_fn=<SumBackward1>)\n","tensor([[ 5.7322,  6.7322,  7.7322,  8.7322,  9.7322],\n","        [21.0000, 22.0000, 23.0000, 24.0000, 25.0000],\n","        [36.0000, 37.0000, 38.0000, 39.0000, 40.0000]],\n","       grad_fn=<SumBackward1>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-tv5cYDgSvmE","colab_type":"code","outputId":"dd4ee352-2974-4929-915e-4bae784e196a","executionInfo":{"status":"ok","timestamp":1540059751466,"user_tz":-420,"elapsed":1082,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["attn = Attention(5, batch_first=False)\n","print(attn)\n","\n","# batch ID is on dim 1\n","batch = T.FloatTensor([ [ [ 1,  2,  3,  4,  5],\n","                          [16, 17, 18, 19, 20],\n","                          [31, 32, 33, 34, 35] ],\n","                       \n","                        [ [ 6,  7,  8,  9, 10],\n","                          [21, 22, 23, 24, 25],\n","                          [36, 37, 38, 39, 40] ],\n","                       \n","                        [ [11, 12, 13, 14, 15], \n","                          [26, 27, 28, 29, 30],\n","                          [41, 42, 43, 44, 45] ]\n","                      ])\n","print(batch)\n","\n","out = attn(batch)\n","print(out)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Attn(dim_input=5, batch_first=False)\n","tensor([[[ 1.,  2.,  3.,  4.,  5.],\n","         [16., 17., 18., 19., 20.],\n","         [31., 32., 33., 34., 35.]],\n","\n","        [[ 6.,  7.,  8.,  9., 10.],\n","         [21., 22., 23., 24., 25.],\n","         [36., 37., 38., 39., 40.]],\n","\n","        [[11., 12., 13., 14., 15.],\n","         [26., 27., 28., 29., 30.],\n","         [41., 42., 43., 44., 45.]]])\n","tensor([[ 5.8593,  6.8593,  7.8593,  8.8593,  9.8593],\n","        [21.0000, 22.0000, 23.0000, 24.0000, 25.0000],\n","        [36.0000, 37.0000, 38.0000, 39.0000, 40.0000]],\n","       grad_fn=<SumBackward1>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jpEnwyAl0ngw","colab_type":"text"},"source":["### RNNs with Attention Mechanism"]},{"cell_type":"markdown","metadata":{"id":"EuM4qnwD-xbt","colab_type":"text"},"source":["The attention mechanism is particularly useful when applying on top of the RNNs. This is because if we compute the weighted average of the internal states, we can capture the importance of each sweeping step. Doing so enables us to discover which parts of the input sequence are significant for classification.\n","\n","For example, if we apply RNNs with attention mechanism on a sequence of words in intent classification, we will now know which words determines the output class."]},{"cell_type":"code","metadata":{"id":"_FGp22yB0tij","colab_type":"code","colab":{}},"source":["class AttnGRU(N.GRU):\n","    \n","    def __init__(self, *args, **kwargs):\n","        super(AttnGRU, self).__init__(*args, **kwargs)\n","        \n","        if self.bidirectional:\n","            self.attention_size = 2 * self.hidden_size\n","        else:\n","            self.attention_size = self.hidden_size\n","        self.attn = Attention(self.attention_size, batch_first=self.batch_first)\n","        \n","    def forward(self, vecs):\n","        ctxvecs, lasthids = super(AttnGRU, self).forward(vecs)\n","        if isinstance(ctxvecs, N.utils.rnn.PackedSequence):\n","            (ctxvecs, padded_lens) = N.utils.rnn.pad_packed_sequence(\n","                ctxvecs, self.batch_first\n","            )\n","            attnvecs = self.attn(ctxvecs)\n","        else:\n","            attnvecs = self.attn(ctxvecs)\n","        return attnvecs, lasthids"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6cc6LxQxFsC","colab_type":"code","outputId":"7bcd4828-f1a9-423f-dd16-8962d7e17f60","executionInfo":{"status":"ok","timestamp":1540059754920,"user_tz":-420,"elapsed":1235,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":476}},"source":["attngru = AttnGRU(5, 4, 1, batch_first=True, bidirectional=True)\n","print(attngru)\n","\n","# batch ID is on dim 0\n","batch = T.FloatTensor([ [ [ 1,  2,  3,  4,  5],\n","                          [ 6,  7,  8,  9, 10],\n","                          [11, 12, 13, 14, 15] ],\n","                       \n","                        [ [16, 17, 18, 19, 20],\n","                          [21, 22, 23, 24, 25],\n","                          [26, 27, 28, 29, 30] ],\n","                       \n","                        [ [31, 32, 33, 34, 35],\n","                          [36, 37, 38, 39, 40],\n","                          [41, 42, 43, 44, 45] ]\n","                      ])\n","print(batch); print()\n","\n","attnvecs, lasthids = attngru(batch)\n","print(attnvecs)\n","print(lasthids)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["AttnGRU(\n","  5, 4, batch_first=True, bidirectional=True\n","  (attn): Attn(dim_input=8, batch_first=True)\n",")\n","tensor([[[ 1.,  2.,  3.,  4.,  5.],\n","         [ 6.,  7.,  8.,  9., 10.],\n","         [11., 12., 13., 14., 15.]],\n","\n","        [[16., 17., 18., 19., 20.],\n","         [21., 22., 23., 24., 25.],\n","         [26., 27., 28., 29., 30.]],\n","\n","        [[31., 32., 33., 34., 35.],\n","         [36., 37., 38., 39., 40.],\n","         [41., 42., 43., 44., 45.]]])\n","\n","tensor([[ 0.7376, -0.0808,  0.1260, -0.7496, -0.1842, -0.0508,  0.0837,  0.9887],\n","        [ 0.9994,  0.0000,  0.9693, -0.6933, -0.0003, -0.0002,  0.0001,  1.0000],\n","        [ 1.0000,  0.0000,  0.9996, -0.5626, -0.0000, -0.0000,  0.0000,  1.0000]],\n","       grad_fn=<SumBackward1>)\n","tensor([[[ 0.9904, -0.0810,  0.7203, -0.9055],\n","         [ 1.0000,  0.0000,  0.9960, -0.8620],\n","         [ 1.0000,  0.0000,  0.9999, -0.7365]],\n","\n","        [[-0.4577, -0.1256,  0.2146,  0.9688],\n","         [-0.0007, -0.0005,  0.0003,  1.0000],\n","         [-0.0000, -0.0000,  0.0000,  1.0000]]], grad_fn=<ViewBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HuDPtEOK4Jac","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GH0plctEZG9u","colab_type":"text"},"source":["## Document Classification"]},{"cell_type":"markdown","metadata":{"id":"eEHg6cSAZNI9","colab_type":"text"},"source":["### Batch Preparation"]},{"cell_type":"code","metadata":{"id":"QIXEgRkPAVYJ","colab_type":"code","colab":{}},"source":["def pack_batch(mats, batch_first=True):\n","    maxlen = max(mat.size(0) for mat in mats)\n","    maxdim = max(mat.size(1) for mat in mats)\n","    items = [ (mats[idx].size(0), idx)\n","              for idx in range(len(mats)) ]\n","    items.sort(reverse=True)\n","    \n","    if batch_first:\n","        padded_mats = T.zeros(len(mats), maxlen, maxdim)\n","    else:\n","        padded_mats = T.zeros(maxlen, len(mats), maxdim)\n","    padded_lens, idxorder = [], []\n","    for (i, (matlen, idx)) in enumerate(items):\n","        mat = mats[idx]\n","        matlen = mat.size(0)\n","        padded_lens.append(matlen)\n","        idxorder.append(idx)\n","        if batch_first:\n","            padded_mats[i, : mat.size(0), : mat.size(1)] = mat\n","        else:\n","            padded_mats[: mat.size(0), i, : mat.size(1)] = mat\n","    return (\n","        N.utils.rnn.pack_padded_sequence(\n","            padded_mats, padded_lens, batch_first\n","        ), \n","        idxorder\n","    )\n","\n","def unpack_batch(batch, idxorder, batch_first=True):\n","    (padded_mats, padded_lens) = N.utils.rnn.pad_packed_sequence(batch, batch_first)\n","    return [ padded_mats[idx][: padded_lens[idx]] for idx in idxorder ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4TbrM0odAlDj","colab_type":"code","outputId":"34c20180-9fcb-4fc8-ad18-a9e82c82d4d7","executionInfo":{"status":"ok","timestamp":1540059758769,"user_tz":-420,"elapsed":2097,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["mats = [ T.randn(2, 5),\n","         T.randn(4, 5),\n","         T.randn(3, 5) ]\n","\n","batch, idxorder = pack_batch(mats, True)\n","print(batch)\n","print(idxorder)\n","print()\n","\n","recov_mats = unpack_batch(batch, idxorder, True)\n","print(recov_mats)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["PackedSequence(data=tensor([[ 1.5795, -0.3975, -1.1916, -0.6541,  0.0646],\n","        [ 0.6686, -0.7877, -0.0330, -0.6806, -1.7733],\n","        [-0.0844,  0.3017,  0.0828, -1.5118, -2.1294],\n","        [ 1.0079, -0.3018, -1.2249,  0.6197,  1.4975],\n","        [-1.2123, -1.5901, -2.1531, -1.1153,  0.1499],\n","        [ 1.1749,  0.2001,  0.0212, -1.8210,  2.2219],\n","        [-0.5592,  0.9031,  1.1478,  0.2576, -0.3792],\n","        [ 0.1089, -0.0797, -2.4893, -1.4018,  0.5981],\n","        [ 0.3485, -1.8608,  1.3713,  0.4281, -0.5659]]), batch_sizes=tensor([3, 3, 2, 1]))\n","[1, 2, 0]\n","\n","[tensor([[ 0.6686, -0.7877, -0.0330, -0.6806, -1.7733],\n","        [-1.2123, -1.5901, -2.1531, -1.1153,  0.1499],\n","        [ 0.1089, -0.0797, -2.4893, -1.4018,  0.5981]]), tensor([[-0.0844,  0.3017,  0.0828, -1.5118, -2.1294],\n","        [ 1.1749,  0.2001,  0.0212, -1.8210,  2.2219]]), tensor([[ 1.5795, -0.3975, -1.1916, -0.6541,  0.0646],\n","        [ 1.0079, -0.3018, -1.2249,  0.6197,  1.4975],\n","        [-0.5592,  0.9031,  1.1478,  0.2576, -0.3792],\n","        [ 0.3485, -1.8608,  1.3713,  0.4281, -0.5659]])]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7ahNLKzg4jBz","colab_type":"text"},"source":["### Neural Architecture"]},{"cell_type":"markdown","metadata":{"id":"byX4J1SJ_wAm","colab_type":"text"},"source":["Hierarchical Attention Networks (HANs) for document classification consists of two layers of RNNs with attention mechanism.\n","\n","- **Layer 1:** RNN with attention mechanism for word sequence. This layer will capture the significance of each word. It produces a sentence embedding vector.\n","- **Layer 2:** RNN with attention mechanism for sentence sequence. This layer will capture the significance of each sentence. It produces a document embedding vector."]},{"cell_type":"code","metadata":{"id":"QHa5hYYdYZmg","colab_type":"code","colab":{}},"source":["class DocumentClassifier(N.Module):\n","    \n","    def __init__(\n","        self, no_words, no_categories, dim_word, dim_wordhid, dim_senthid, \n","        layers_word=1, layers_sent=1, dropout=0.0\n","    ):\n","        super(DocumentClassifier, self).__init__()\n","        self._no_words = no_words\n","        self._no_categories = no_categories\n","        self._dim_word = dim_word\n","        self._dim_wordhid = dim_wordhid\n","        self._layers_word = layers_word\n","        self._dim_senthid = dim_senthid\n","        self._layers_sent = layers_sent\n","        self._dropout = dropout\n","        self._batch_first = True\n","        \n","        self._wordemb = N.Embedding(self._no_words, self._dim_word)\n","        self._attngru_word = AttnGRU(\n","            self._dim_word, self._dim_wordhid, self._layers_word,\n","            batch_first=self._batch_first,\n","            dropout=self._dropout, bidirectional=True\n","        )\n","        self._attngru_sent = AttnGRU(\n","            self._attngru_word.attention_size, self._dim_senthid, self._layers_sent,\n","            batch_first=self._batch_first,\n","            dropout=self._dropout, bidirectional=True\n","        )\n","        self._hidden = N.Linear(self._attngru_sent.attention_size, self._no_categories)\n","        # self._log_softmax = N.LogSoftmax(dim=0)\n","        \n","    def forward(self, docs):\n","        \n","        # Prepare mapping from each document to its member sentences\n","        doclens, begpos, doc2idxs = [], 0, []\n","        for (docidx, doc) in enumerate(docs):\n","            endpos = begpos + len(doc)\n","            doclens.append((begpos, endpos, docidx))\n","            begpos = endpos\n","            doc2idxs.append([])\n","        \n","        # HAN Layer 1: Sweep RNN on each word and compute the attention\n","        # Compute the summary vector of each sentence\n","        sentmats = [ self._wordemb(T.LongTensor(wordidxseq))\n","                     for wordidxseqs in docs\n","                     for wordidxseq in wordidxseqs ]\n","        batch, idxorder = pack_batch(sentmats, batch_first=self._batch_first)\n","        sentvecs, lasthids = self._attngru_word(batch)\n","        \n","        # Map the summary vectors back to their document in the original order\n","        for i in range(len(idxorder)):\n","            sentidx = idxorder[i]\n","            for (begpos, endpos, docidx) in doclens:\n","                if begpos <= sentidx < endpos:\n","                    doc2idxs[docidx].append((sentidx, i))\n","                    break\n","        for doc in doc2idxs:\n","            doc.sort()\n","        \n","        # HAN Layer 2: Sweep RNN on each sentence and compute the attention\n","        # Compute the summary vector of each document\n","        docmats = []\n","        for doc in doc2idxs:\n","            idxs = T.LongTensor([i for (sentidx, i) in doc])\n","            docmat = sentvecs[idxs]\n","            docmats.append(docmat)\n","        batch, idxorder = pack_batch(docmats, batch_first=self._batch_first)\n","        docvecs, lasthids = self._attngru_sent(batch)\n","        \n","        # Make prediction and relocate the results back to the original order\n","        # outvec = self._log_softmax(self._hidden(docvecs))\n","        outvec = self._hidden(docvecs)\n","        orgidxs = T.LongTensor(idxorder)\n","        return outvec[orgidxs]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCYZOPEWkv7H","colab_type":"code","colab":{}},"source":["doccls_model = DocumentClassifier(no_words, no_categories, 32, 16, 16, layers_word=1, layers_sent=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jS6MEmge9Zwt","colab_type":"code","outputId":"43ec676e-0ae5-4786-b21b-8e376c7a0a43","executionInfo":{"status":"ok","timestamp":1540059763657,"user_tz":-420,"elapsed":1577,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["docs = [training_set[i][0] for i in range(3)]\n","goldvec = T.LongTensor([training_set[i][1] for i in range(3)])\n","outvec = doccls_model(docs)\n","print(outvec)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0875, -0.0036],\n","        [-0.0110, -0.0131],\n","        [ 0.1394,  0.0006]], grad_fn=<TakeBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sv7Dq1rc9fer","colab_type":"code","outputId":"48427530-6f20-4f13-87d2-f04c32ee40b5","executionInfo":{"status":"ok","timestamp":1540059764901,"user_tz":-420,"elapsed":1168,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(goldvec)\n","\n","loss_fn = N.CrossEntropyLoss()\n","loss = loss_fn(outvec, goldvec)\n","print(loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([0, 0, 0])\n","tensor(0.6556, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VxJLTTkBk46e","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7r8No8Mzk5x_","colab_type":"text"},"source":["### Batch Training"]},{"cell_type":"code","metadata":{"id":"JUfyAb7tk8Tt","colab_type":"code","colab":{}},"source":["def batch_ranges(no_items, batch_size):\n","    batches = []\n","    begpos = 0\n","    for i in range(no_items // batch_size):\n","        endpos = min((i + 1) * batch_size, no_items)\n","        batches.append(range(begpos, endpos))\n","        begpos = endpos\n","    return batches\n","\n","def train_model(\n","    doccls_model, training_data, epochs, loss_fn, optimizer, batch_size=10\n","):\n","    no_samples = len(training_data)\n","    loss_history = []\n","    \n","    for i in range(epochs):\n","        total_loss = 0.0\n","        random.shuffle(training_data)\n","        batches = batch_ranges(len(training_data), batch_size)\n","        \n","        for batch in tqdm(batches):\n","            docs = [training_data[i][0] for i in batch]\n","            clsidxs = [training_data[i][1] for i in batch]\n","            \n","            pred_clsvec = doccls_model.forward(docs)   # Perform prediction\n","            gold_clsvec = T.LongTensor(clsidxs)        # Gold standard\n","\n","            loss = loss_fn(pred_clsvec, gold_clsvec)\n","            total_loss += loss.item()\n","\n","            optimizer.zero_grad()      # Clear gradient cache\n","            loss.backward()            # Perform backpropagation\n","            optimizer.step()           # Update the model parameters\n","        \n","        loss_history.append(total_loss / len(batches))\n","        \n","    # Plot the loss history with Matplotlib\n","    epoch_count = range(1, epochs + 1)\n","    plt.plot(epoch_count, loss_history, 'b--')\n","    plt.legend(['Training Loss'])\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z8RRELAZAWSI","colab_type":"text"},"source":["<font color='red'>**WARNING: IT MAY TAKE SOME TIME AS YOU ARE TRAINING A MODEL ON A DATASET CONTAINING OVER 1.5M WORDS.**</font>"]},{"cell_type":"code","metadata":{"id":"3QTpo_H2kgYs","colab_type":"code","outputId":"912a045c-bd09-4694-cdd3-fc9d3b5a673f","executionInfo":{"status":"ok","timestamp":1540069384131,"user_tz":-420,"elapsed":4702238,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":623}},"source":["doccls_model = DocumentClassifier(no_words, no_categories, 32, 16, 16, layers_word=1, layers_sent=1)\n","\n","if use_cuda and T.cuda.is_available():\n","    doccls_model.cuda()\n","\n","epochs = 20\n","learning_rate = 0.001\n","\n","loss_fn = N.CrossEntropyLoss()\n","optimizer = O.Adam(doccls_model.parameters(), lr=learning_rate)\n","train_model(doccls_model, training_set, epochs, loss_fn, optimizer, batch_size=4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 400/400 [03:51<00:00,  1.68it/s]\n","100%|██████████| 400/400 [03:47<00:00,  1.77it/s]\n","100%|██████████| 400/400 [03:48<00:00,  1.66it/s]\n","100%|██████████| 400/400 [03:52<00:00,  1.40it/s]\n","100%|██████████| 400/400 [03:52<00:00,  1.95it/s]\n","100%|██████████| 400/400 [03:53<00:00,  1.63it/s]\n","100%|██████████| 400/400 [03:52<00:00,  2.06it/s]\n","100%|██████████| 400/400 [03:54<00:00,  1.80it/s]\n","100%|██████████| 400/400 [03:57<00:00,  1.25it/s]\n","100%|██████████| 400/400 [03:55<00:00,  1.89it/s]\n","100%|██████████| 400/400 [03:56<00:00,  1.62it/s]\n","100%|██████████| 400/400 [03:55<00:00,  1.98it/s]\n","100%|██████████| 400/400 [03:57<00:00,  1.53it/s]\n","100%|██████████| 400/400 [03:58<00:00,  2.00it/s]\n","100%|██████████| 400/400 [03:56<00:00,  1.86it/s]\n","100%|██████████| 400/400 [03:58<00:00,  1.41it/s]\n","100%|██████████| 400/400 [03:56<00:00,  1.28it/s]\n","100%|██████████| 400/400 [03:58<00:00,  1.95it/s]\n","100%|██████████| 400/400 [03:59<00:00,  2.16it/s]\n","100%|██████████| 400/400 [03:56<00:00,  1.81it/s]\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f055a375e10>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"LV4Or5sN03C-","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KCvWf42N04X9","colab_type":"text"},"source":["### Classification"]},{"cell_type":"code","metadata":{"id":"cUTCTTxy09Y7","colab_type":"code","colab":{}},"source":["def classify(doccls_model, doc):\n","    wordidxseqs = sents2idxseqs(doc)\n","    pred_clsvec = doccls_model([wordidxseqs])\n","    return pred_clsvec.argmax(dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mlvg9FSy1qe4","colab_type":"code","outputId":"6e579c80-9000-41ea-c0c7-9d60602885f6","executionInfo":{"status":"ok","timestamp":1540069388050,"user_tz":-420,"elapsed":1092,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cls = classify(doccls_model, corpus[0][0])\n","print(cls)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([0])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kf5zugQ63WVX","colab_type":"text"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IKOey8Yi3Xbs","colab_type":"text"},"source":["### Testing Procedure"]},{"cell_type":"code","metadata":{"id":"LxcmihLo3cLq","colab_type":"code","colab":{}},"source":["def test_model(doccls_model, testing_data):\n","    no_correct = 0.0\n","    no_total = 0.0\n","\n","    for (doc, clsidx) in tqdm(testing_data):\n","        pred_clsvec = doccls_model([doc])\n","        pred_clsidx = pred_clsvec.argmax(dim=1)[0]\n","        if pred_clsidx == clsidx: no_correct += 1.0\n","        no_total += 1.0\n","\n","    acc = 100 * no_correct / no_total\n","    \n","    print('\\nAccuracy = {}'.format(acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMFGYJz_ypVy","colab_type":"code","outputId":"ddd81f56-2c90-41a1-c8ad-b50278ab13d4","executionInfo":{"status":"ok","timestamp":1540069408567,"user_tz":-420,"elapsed":18898,"user":{"displayName":"Prachya Boonkwan","photoUrl":"","userId":"08646984894086426070"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print('Accuracy:')\n","test_model(doccls_model, testing_set)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  1%|          | 3/400 [00:00<00:17, 23.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Accuracy:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 400/400 [00:17<00:00, 23.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Accuracy = 81.5\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"sbrU-cNzBGrV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}